{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_pos_dict = {\"conv1_1\" : 1, \"conv2_1\" : 4, \"conv3_1\" : 7, \"conv4_1\" : 11, \"conv5_1\" : 15}\n",
    "\n",
    "content_layer = \"conv3_1\"\n",
    "content_loss_weight = 0.1\n",
    "style_loss_weights = {\"conv1_1\" : 3000, \"conv2_1\" : 750, \"conv3_1\" : 250, \"conv4_1\" : 100, \"conv5_1\" : 50}\n",
    "tv_loss_weight = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEANS = [103.939, 116.779, 123.68]\n",
    "# IMAGENET_MEANS = [0.40760392, 0.45795686, 0.48501961]\n",
    "\n",
    "def process_image(img):\n",
    "    processed_image = np.array(img).astype(np.float32)\n",
    "#     processed_image /= 255\n",
    "    for x in range(3):\n",
    "        processed_image[:, :, x] -= IMAGENET_MEANS[x]\n",
    "    return processed_image\n",
    "        \n",
    "def restore_image(img):\n",
    "    restored_image = np.array(img)\n",
    "    for x in range(3):\n",
    "        restored_image[:, :, x] += IMAGENET_MEANS[x]\n",
    "    restored_image.clip(0, 255)\n",
    "#     restored_image.clip(0, 1)\n",
    "#     restored_image *= 255\n",
    "    return restored_image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_source = cv2.imread(\"../../data/images/Amsterdam.jpg\", 1)\n",
    "content_source = cv2.resize(content_source, (224, 224))\n",
    "\n",
    "style_source = cv2.imread(\"../../data/images/VanGogh.jpg\", 1)\n",
    "style_source = cv2.resize(style_source, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_content = tf.constant(process_image(content_source))\n",
    "target_style = tf.constant(process_image(style_source))\n",
    "recovered_image = tf.Variable(tf.random_normal([1, 224, 224, 3]), name=\"recovered_image\", trainable=True)\n",
    "concatenated_input = tf.concat([tf.expand_dims(target_content, axis=0), \n",
    "                                tf.expand_dims(target_style, axis=0), \n",
    "                                recovered_image\n",
    "                               ], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16 = tf.contrib.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', \n",
    "                                                  input_tensor=concatenated_input, input_shape=None)\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_content_loss(layer):\n",
    "    content_embeddings = vgg16.layers[layer_pos_dict[layer]].output\n",
    "    return tf.reduce_sum(tf.squared_difference(content_embeddings[0, :, :, :], content_embeddings[2, :, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(embedding):\n",
    "    filters_first = tf.transpose(embedding, perm=[2, 0, 1])\n",
    "    filters_flatten = tf.contrib.keras.backend.batch_flatten(filters_first)\n",
    "    gram = tf.matmul(filters_flatten, filters_flatten, transpose_b=True)\n",
    "    return gram\n",
    "\n",
    "def get_style_loss(layer):\n",
    "    style_embeddings = vgg16.layers[layer_pos_dict[layer]].output\n",
    "    \n",
    "    embedding_shape = style_embeddings.get_shape().as_list()\n",
    "    layer_width = embedding_shape[1]\n",
    "    layer_height = embedding_shape[2]\n",
    "    n_filters = embedding_shape[3]\n",
    "    style_norm = 4 * (n_filters * layer_width * layer_height) ** 2\n",
    "    \n",
    "    target_gram = gram_matrix(style_embeddings[1, :, :, :])\n",
    "    recovered_gram = gram_matrix(style_embeddings[2, :, :, :])\n",
    "    \n",
    "    return tf.reduce_sum(tf.squared_difference(target_gram, recovered_gram)) / style_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_total_variation_loss(x):\n",
    "    width, height = 224, 224\n",
    "    width_offset = tf.square(x[:, :width - 1, :height - 1, :] - x[:, 1:, :height - 1, :])\n",
    "    height_offset = tf.square(x[:, :width - 1, :height - 1, :] - x[:, :width - 1, 1:, :])\n",
    "    return tf.reduce_sum(width_offset + height_offset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_loss = content_loss_weight * get_content_loss(content_layer)\n",
    "style_loss = tf.Variable(0.)\n",
    "for style_layer in style_loss_weights:\n",
    "    style_loss += style_loss_weights[style_layer] * get_style_loss(style_layer)\n",
    "tv_loss = tv_loss_weight * get_total_variation_loss(recovered_image)\n",
    "\n",
    "total_loss = content_loss + style_loss + tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = tf.train.AdamOptimizer(3e-1).minimize(total_loss, var_list=[recovered_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        current_content_loss, current_style_loss, current_tv_loss, current_total_loss, _ = \\\n",
    "        sess.run([content_loss, style_loss, tv_loss, total_loss, adam])\n",
    "        print(i, current_content_loss, current_style_loss, current_tv_loss, current_total_loss)\n",
    "    final_image = recovered_image.eval()[0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(content_source[:, :, [2, 1, 0]])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(style_source[:, :, [2, 1, 0]])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(restore_image(final_image)[:, :, [2, 1, 0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl_env]",
   "language": "python",
   "name": "conda-env-dl_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
