{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16 = tf.contrib.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=None)\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_pos_dict = {\"conv1_2\" : 2, \"conv2_2\" : 5, \"conv3_2\" : 8, \"conv4_2\" : 12, \"conv5_2\" : 16}\n",
    "layer_pick = \"conv2_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"../../data/images/VanGogh.jpg\", 1)\n",
    "img = cv2.resize(img, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEANS = [103.939, 116.779, 123.68]\n",
    "\n",
    "def process_image(img):\n",
    "    processed_image = np.array(img).astype(np.float32)\n",
    "    for x in range(3):\n",
    "        processed_image[:, :, x] -= IMAGENET_MEANS[x]\n",
    "    return processed_image\n",
    "        \n",
    "def restore_image(img):\n",
    "    restored_image = np.array(img)\n",
    "    for x in range(3):\n",
    "        restored_image[:, :, x] += IMAGENET_MEANS[x]\n",
    "    restored_image.clip(0, 255)\n",
    "    return restored_image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_image = tf.placeholder(tf.float32, shape=(224, 224, 3), name=\"target_image\")\n",
    "recovered_image = tf.Variable(tf.random_normal([224, 224, 3]), name=\"recovered_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(image):\n",
    "    last_layer = tf.expand_dims(image, axis=0)\n",
    "    for i in range(1, layer_pos_dict[layer_pick] + 1):\n",
    "        next_layer = vgg16.layers[i](last_layer)\n",
    "        last_layer = next_layer\n",
    "    return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(embedding):\n",
    "    filters_first = tf.transpose(embedding, perm=[3, 0, 1, 2])\n",
    "    filters_flatten = tf.contrib.keras.backend.batch_flatten(filters_first)\n",
    "    gram = tf.matmul(filters_flatten, filters_flatten, transpose_b=True)\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_gram = gram_matrix(get_embedding(target_image))\n",
    "recovered_gram = gram_matrix(get_embedding(recovered_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_loss = tf.reduce_sum(tf.squared_difference(target_gram, recovered_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = tf.train.AdamOptimizer(1e-4).minimize(style_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed_dict = {target_image : process_image(img)}\n",
    "    sess.run(adam, feed_dict=feed_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl_env]",
   "language": "python",
   "name": "conda-env-dl_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
