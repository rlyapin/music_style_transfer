{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_pos_dict = {\"conv1_2\" : 2, \"conv2_2\" : 5, \"conv3_2\" : 8, \"conv4_2\" : 12, \"conv5_2\" : 16}\n",
    "layer_pick = \"conv5_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"../../data/images/VanGogh.jpg\", 1)\n",
    "img = cv2.resize(img, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEANS = [103.939, 116.779, 123.68]\n",
    "\n",
    "def process_image(img):\n",
    "    processed_image = np.array(img).astype(np.float32)\n",
    "    for x in range(3):\n",
    "        processed_image[:, :, x] -= IMAGENET_MEANS[x]\n",
    "    return processed_image\n",
    "        \n",
    "def restore_image(img):\n",
    "    restored_image = np.array(img)\n",
    "    for x in range(3):\n",
    "        restored_image[:, :, x] += IMAGENET_MEANS[x]\n",
    "    restored_image.clip(0, 255)\n",
    "    return restored_image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broken approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below does not really work: the problem arises when I try to rechain the layers from vgg16 model (i.e. the outputs of intermediary layers are different when generated with the same block of code and the same fixed input) - the problem may lie in the fact that rechaining keras layers results in them having several input and output nodes - and tensorflow may get confused collecting them. I decided to leave the code as it is in case I ever would like to investigate that behaviour further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16 = tf.contrib.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=None)\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_image = tf.placeholder(tf.float32, shape=(224, 224, 3), name=\"target_image\")\n",
    "recovered_image = tf.Variable(tf.random_normal([224, 224, 3]), name=\"recovered_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(image):\n",
    "    last_layer = tf.expand_dims(image, axis=0)\n",
    "    for i in range(1, layer_pos_dict[layer_pick] + 1):\n",
    "        next_layer = vgg16.layers[i](last_layer)\n",
    "        last_layer = next_layer\n",
    "    return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(embedding):\n",
    "    filters_first = tf.transpose(embedding, perm=[3, 0, 1, 2])\n",
    "    filters_flatten = tf.contrib.keras.backend.batch_flatten(filters_first)\n",
    "    gram = tf.matmul(filters_flatten, filters_flatten, transpose_b=True)\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_gram = gram_matrix(get_embedding(target_image))\n",
    "recovered_gram = gram_matrix(get_embedding(recovered_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_loss = tf.reduce_sum(tf.squared_difference(target_gram, recovered_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = tf.train.AdamOptimizer(1e-4).minimize(style_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed_dict = {target_image : process_image(img)}\n",
    "    sess.run(adam, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is supposed to properly reconstruct gram matrices for filters inside convolutional layers of vgg16. The main difference from broken section lies in specifying input tensor to vgg16 model - this way keras layers inside it have only one input and output nodes and I can properly extract the embeddings I need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_image = tf.constant(process_image(img))\n",
    "recovered_image = tf.Variable(tf.random_normal([1, 224, 224, 3]), name=\"recovered_image\", trainable=True)\n",
    "concatenated_input = tf.concat([tf.expand_dims(target_image, axis=0), recovered_image], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16 = tf.contrib.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', \n",
    "                                                  input_tensor=concatenated_input, input_shape=None)\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = vgg16.layers[layer_pos_dict[layer_pick]].output\n",
    "target_embeddings = embeddings[0, :, :, :]\n",
    "recovered_embeddings = embeddings[1, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(embedding):\n",
    "    filters_first = tf.transpose(embedding, perm=[2, 0, 1])\n",
    "    filters_flatten = tf.contrib.keras.backend.batch_flatten(filters_first)\n",
    "    gram = tf.matmul(filters_flatten, filters_flatten, transpose_b=True)\n",
    "    return gram\n",
    "\n",
    "target_gram = gram_matrix(target_embeddings)\n",
    "recovered_gram = gram_matrix(recovered_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_shape = embeddings.get_shape().as_list()\n",
    "layer_width = embedding_shape[1]\n",
    "layer_height = embedding_shape[2]\n",
    "n_filters = embedding_shape[3]\n",
    "style_norm = 4 * (n_filters * layer_width * layer_height) ** 2\n",
    "style_loss = tf.reduce_sum(tf.squared_difference(target_gram, recovered_gram)) / style_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    width, height = 224, 224\n",
    "    width_offset = tf.square(x[:, :width - 1, :height - 1, :] - x[:, 1:, :height - 1, :])\n",
    "    height_offset = tf.square(x[:, :width - 1, :height - 1, :] - x[:, :width - 1, 1:, :])\n",
    "    return tf.reduce_sum(width_offset + height_offset)\n",
    "\n",
    "# for conv3_2 layer: lr=3e+1, beta=10^(-8)\n",
    "beta = 10 ** (-8)\n",
    "total_loss = style_loss + beta * total_variation_loss(recovered_image)\n",
    "adam = tf.train.AdamOptimizer(3e+1).minimize(total_loss, var_list=[recovered_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10):\n",
    "        current_style_loss, current_total_loss, _ = sess.run([style_loss, total_loss, adam])\n",
    "        print(i, current_style_loss, current_total_loss)\n",
    "    final_image = recovered_image.eval()[0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(restore_image(final_image)[:, :, [2, 1, 0]])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img[:, :, [2, 1, 0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl_env]",
   "language": "python",
   "name": "conda-env-dl_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
